For exercise 3, I developed both a single and multi objective EA suited for uniform-constraint optimisation of data through both
generative AI and manual coding optimisations.

The process begins by using a baseline (1+lambda) population based approach then methodically incorporating methods to 
introduce diversity to the results, while also avoiding early convergence, which is a common issue when dealing with evolutionary
binary sets of data. 

For the single objective EA, I implemented a population based method using random replacement to maintain population diversity
while also encouraging exploration across different solution sets. Conversely, the multi objective EA extended this further to
evaluate for solutions on both coverage and cost, storing non dominated solutions in Pareto storage. A uniform selection 
strategy (tournament) was use to balance sampling across this archive and prevented the algorithm from over focusing on a singular 
region of the Pareto.

When looking at the results, a clear performance gap between the two approaches is shown. For example, considering the instance
2100 with population 10, the single objective EA consistently converged to a best fitness of ~38 across 25 evaluations, showcasing
both reliability and stability in convergence within that 10,000 evaluation budget. Contrastingly, the multi-objective EA reached
a lower fitness of ~41 but showed slower convergence and greater fluctuations across early evaluations, likely attributed to 
the challenge of balancing of both Pareto diversity and convergence speed. Many runs showed early stagnation, highlighting that
while the multi objective approach promotes exploration, it struggles to efficiently guide the search towards feasible solutions
within that fixed budget.

Overall, the single objective EA proved more effective for this instance, shown by its production of reliable, fast and stable
optimisatoins. The multi objective approach, while ambitious in principle, requires further refinement, such as archive pruning or
objective scaling of sorts, to better balance exploration and convergence efficiency.

a1886664 - Manith Kandanearachchi